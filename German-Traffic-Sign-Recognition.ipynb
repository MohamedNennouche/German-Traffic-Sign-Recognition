{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Input, Dense, MaxPool2D, BatchNormalization, GlobalAvgPool2D, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "#### Preparing training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(path_to_data, path_to_save_train, path_to_save_val, split_size=0.1) :\n",
    "    folders = os.listdir(path_to_data) # la liste des dossiers disponible au chemin donné\n",
    "    for folder in folders :\n",
    "        full_path = os.path.join(path_to_data, folder) # pour avoir le chemin complet en ajoutant le nom des dossiers\n",
    "        images_paths = glob.glob(os.path.join(full_path, '*.png')) # ca prend tous les fichiers à l'intérieur du dossier et les télécharge (le join il va a chaque fois ajouter le path du dossier et ajoutant le nom du fichier) ca nous retourne une liste d'images\n",
    "        x_train, x_val = train_test_split(images_paths, test_size=split_size) # split en train et validation\n",
    "\n",
    "        for x in x_train : \n",
    "            path_to_folder = os.path.join(path_to_save_train, folder) # pour recréer les même dossier que dans le dossier de base\n",
    "            if not os.path.isdir(path_to_folder) : \n",
    "                os.makedirs(path_to_folder) # si il n'existe pas il le crée\n",
    "            shutil.copy(x, path_to_folder)\n",
    "        \n",
    "        for x in x_val : \n",
    "            path_to_folder = os.path.join(path_to_save_val, folder) # pour recréer les même dossier que dans le dossier de base\n",
    "            if not os.path.isdir(path_to_folder) : \n",
    "                os.makedirs(path_to_folder) # si il n'existe pas il le crée\n",
    "            shutil.copy(x, path_to_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__' :\n",
    "    path_to_data = './Train'\n",
    "    path_to_save_train = './Training_data/train'\n",
    "    path_to_save_val = './Training_data/val'\n",
    "    split_data(path_to_data, path_to_save_train, path_to_save_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the test set\n",
    "Pour avoir les labels de chaque images de test puisqu'elles sont toute dans un même dossier, toute mélangée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_test_set(path_to_images, path_to_csv) :\n",
    "    try : \n",
    "        with open(path_to_csv, 'r') as csvfile : \n",
    "            reader = csv.reader(csvfile, delimiter=',')\n",
    "            for i, row in enumerate(reader) :\n",
    "                if i == 0 : \n",
    "                    continue # pour ne pas prendre en compte la première ligne\n",
    "                img_name = row[-1].replace('Test/','') # prendre la dernière colonne qui est le nom de l'image en enlevant le 'Test/' au début de chaque nom d'image\n",
    "                label = row[-2]\n",
    "\n",
    "                path_to_folder = os.path.join(path_to_images,label) # on crée un dossier avec le nom du label comme le dataset d'entrainement\n",
    "                if not os.path.isdir(path_to_folder) :\n",
    "                    os.makedirs(path_to_folder)\n",
    "\n",
    "                img_full_path = os.path.join(path_to_images, img_name)\n",
    "\n",
    "                shutil.move(img_full_path, path_to_folder) # on va déplacer et pas copier\n",
    "    except : \n",
    "        print(\"On ne peut pas ouvrir le fichier csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On ne peut pas ouvrir le fichier csv\n"
     ]
    }
   ],
   "source": [
    "path_to_images = './Test'\n",
    "path_to_csv = './Test.csv'\n",
    "order_test_set(path_to_images, path_to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau de neurones\n",
    "Maintenant que le dataset est prêt on peut maintenant passer à la conception du modèle de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def streetsigns_model(nbr_classes) :\n",
    "    my_input = Input(shape=(60,60, 3)) # taille moyenne et les 3 canaux RGB\n",
    "    x= Conv2D(32, (3,3), activation='relu')(my_input)\n",
    "    x= Conv2D(64, (3,3), activation='relu')(x)\n",
    "    x= MaxPool2D()(x)\n",
    "    x= BatchNormalization()(x)\n",
    "\n",
    "    x= Conv2D(128, (3,3), activation='relu')(x)\n",
    "    x= MaxPool2D()(x)\n",
    "    x= BatchNormalization()(x)\n",
    "\n",
    "    x= GlobalAvgPool2D()(x)\n",
    "    #x = Flatten()(x)\n",
    "    x= Dense(64, activation='relu')(x)\n",
    "    x= Dense(43, activation='softmax')(x)\n",
    "    model = Model(inputs=my_input, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60, 60, 3)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 28, 28, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 43)                2795      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,067\n",
      "Trainable params: 104,683\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__' :\n",
    "\n",
    "    model = streetsigns_model(43)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ainsi voir ainsi l'architecture de notre réseaux de neurones et surtout voir le nombre de paramètres à entrainer et tout. \n",
    "\n",
    "Remarque : on peut voir qu'on a bien moins de parmaètres avec global Average que Flatten \n",
    "\n",
    "### Générateur de données \n",
    "Pour alimenter notre modèle pour l'entrainement et ensuite le test, il contiendra : \n",
    "- préprocessing de toutes les images\n",
    "- préparation des images pour l'entrainement \n",
    "- acheminement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generators(batch_size, train_data_path, val_data_path, test_data_path) :\n",
    "    preprocessor = ImageDataGenerator(\n",
    "        rescale = 1/255. # pour assurer une division flottante\n",
    "    )\n",
    "\n",
    "    # !très important pour prendre des données en prenant chaque sous dossier comme classe à part entière\n",
    "    train_generator = preprocessor.flow_from_directory(\n",
    "        train_data_path,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(60,60), #resize all images\n",
    "        color_mode = 'rgb', # type d'images\n",
    "        shuffle = True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_generator = preprocessor.flow_from_directory(\n",
    "        val_data_path,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(60,60), #resize all images\n",
    "        color_mode = 'rgb', # type d'images\n",
    "        shuffle = False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_generator = preprocessor.flow_from_directory(\n",
    "        test_data_path,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(60,60), #resize all images\n",
    "        color_mode = 'rgb', # type d'images\n",
    "        shuffle = False,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39209 images belonging to 43 classes.\n",
      "Found 10632 images belonging to 43 classes.\n",
      "Found 12630 images belonging to 43 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './Training_data/train'\n",
    "val_data_path = './Training_data/val'\n",
    "test_data_path = './Test'\n",
    "batch_size = 64\n",
    "\n",
    "train_generator, val_generator, test_generator= create_generators(batch_size, train_data_path, val_data_path, test_data_path)\n",
    "\n",
    "nbr_classes = train_generator.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = streetsigns_model(nbr_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting du modèle et sauvegarde du meilleur \n",
    "\n",
    "On va préparer le modèle comme précédemment et pour la sauvegarde on utilise quelque chose qui s'appelle un Callback qui va permettre de sauvegarder le meilleur modèle.\n",
    "\n",
    "Pour un grand nombre d'époques il y a le Earlystopping pour voir si le modèle ne s'améliore pas on s'arrête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "path_to_save_model = './Models'\n",
    "ckpt_saver = ModelCheckpoint(\n",
    "    path_to_save_model,\n",
    "    monitor='val_accuracy', # sur quoi on se base pour voir le meilleur\n",
    "    mode = 'max', # max de l'accuracy sur la validation\n",
    "    save_best_only = True,\n",
    "    save_freq='epoch', # ne voit qu'à la fin de l'époque\n",
    "    verbose=1\n",
    ") \n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10 # après 10 époques ca change pas on s'arrête\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # on choisit categorical_crossentropy car dans les générateurs on a défini categorical comme class_mode\n",
    "\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 2.4095 - accuracy: 0.3206\n",
      "Epoch 1: val_accuracy improved from -inf to 0.13732, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 537s 874ms/step - loss: 2.4095 - accuracy: 0.3206 - val_loss: 3.0621 - val_accuracy: 0.1373\n",
      "Epoch 2/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 1.1022 - accuracy: 0.6767\n",
      "Epoch 2: val_accuracy improved from 0.13732 to 0.74859, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 569s 928ms/step - loss: 1.1022 - accuracy: 0.6767 - val_loss: 0.7952 - val_accuracy: 0.7486\n",
      "Epoch 3/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.9001\n",
      "Epoch 3: val_accuracy improved from 0.74859 to 0.91930, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 475s 774ms/step - loss: 0.4014 - accuracy: 0.9001 - val_loss: 0.2902 - val_accuracy: 0.9193\n",
      "Epoch 4/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.1736 - accuracy: 0.9609\n",
      "Epoch 4: val_accuracy improved from 0.91930 to 0.95419, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 518s 846ms/step - loss: 0.1736 - accuracy: 0.9609 - val_loss: 0.1750 - val_accuracy: 0.9542\n",
      "Epoch 5/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9791\n",
      "Epoch 5: val_accuracy improved from 0.95419 to 0.97630, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 476s 777ms/step - loss: 0.0970 - accuracy: 0.9791 - val_loss: 0.1041 - val_accuracy: 0.9763\n",
      "Epoch 6/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9848\n",
      "Epoch 6: val_accuracy did not improve from 0.97630\n",
      "613/613 [==============================] - 450s 734ms/step - loss: 0.0682 - accuracy: 0.9848 - val_loss: 0.0866 - val_accuracy: 0.9759\n",
      "Epoch 7/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9893\n",
      "Epoch 7: val_accuracy did not improve from 0.97630\n",
      "613/613 [==============================] - 417s 680ms/step - loss: 0.0476 - accuracy: 0.9893 - val_loss: 0.1215 - val_accuracy: 0.9651\n",
      "Epoch 8/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9890\n",
      "Epoch 8: val_accuracy improved from 0.97630 to 0.98213, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 421s 687ms/step - loss: 0.0456 - accuracy: 0.9890 - val_loss: 0.0586 - val_accuracy: 0.9821\n",
      "Epoch 9/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9918\n",
      "Epoch 9: val_accuracy improved from 0.98213 to 0.98382, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 470s 766ms/step - loss: 0.0338 - accuracy: 0.9918 - val_loss: 0.0471 - val_accuracy: 0.9838\n",
      "Epoch 10/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9940\n",
      "Epoch 10: val_accuracy did not improve from 0.98382\n",
      "613/613 [==============================] - 471s 768ms/step - loss: 0.0261 - accuracy: 0.9940 - val_loss: 0.2754 - val_accuracy: 0.9158\n",
      "Epoch 11/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9904\n",
      "Epoch 11: val_accuracy did not improve from 0.98382\n",
      "613/613 [==============================] - 509s 830ms/step - loss: 0.0352 - accuracy: 0.9904 - val_loss: 0.0756 - val_accuracy: 0.9756\n",
      "Epoch 12/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9933\n",
      "Epoch 12: val_accuracy did not improve from 0.98382\n",
      "613/613 [==============================] - 546s 891ms/step - loss: 0.0244 - accuracy: 0.9933 - val_loss: 0.1056 - val_accuracy: 0.9665\n",
      "Epoch 13/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9937\n",
      "Epoch 13: val_accuracy improved from 0.98382 to 0.98664, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 562s 917ms/step - loss: 0.0223 - accuracy: 0.9937 - val_loss: 0.0393 - val_accuracy: 0.9866\n",
      "Epoch 14/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9947\n",
      "Epoch 14: val_accuracy did not improve from 0.98664\n",
      "613/613 [==============================] - 442s 720ms/step - loss: 0.0199 - accuracy: 0.9947 - val_loss: 0.0602 - val_accuracy: 0.9816\n",
      "Epoch 15/15\n",
      "613/613 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9931\n",
      "Epoch 15: val_accuracy improved from 0.98664 to 0.99135, saving model to .\\Models\n",
      "INFO:tensorflow:Assets written to: .\\Models\\assets\n",
      "613/613 [==============================] - 440s 717ms/step - loss: 0.0244 - accuracy: 0.9931 - val_loss: 0.0289 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c780544c10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Le générateur contient autant les images que les labels\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size,\n",
    "    validation_data = val_generator,\n",
    "    callbacks=[ckpt_saver, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entrainement fait et terminé on a enregistré notre modèle, on pourra alors l'utiliser pour son évaluation en chargeant les données enregistrées dans le dossier Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 60, 60, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 58, 58, 32)        896       \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 28, 28, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 26, 26, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 13, 13, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_average_pooling2d_3   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 43)                2795      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105,067\n",
      "Trainable params: 104,683\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./Models')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 57s 284ms/step - loss: 0.3312 - accuracy: 0.9170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3312202990055084, 0.9170229434967041]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Améliorations du modèle\n",
    "Pour améliorer le modèle on a plusieurs possibilités : \n",
    "- Changer la taille du Batch\n",
    "- Augmenter (ou diminuer) le nombre d'époques\n",
    "- Changer l'architecture du modèle (changer les couches ou en ajouter ou diminuer)\n",
    "- Dans la création du générateur (dans la partie ImageDataGenerator) il y a un certain nombre de techniques pour de la data augmentation (surtout dans le domaine du traitement d'images) \n",
    "- On peut mettre en place plusieurs pré-processeurs pour les adapter, chacun à une partie du problème (train et pas validation et test par exemple) surtout dans le cas de l'augmentation des données (avec des shifts et des zoom) \n",
    "- On peut changer l'optimize en utilisant opitmizer = tf.keras.optimizers.NomOptimizer() et on choisit d'après la documentation qu'on a\n",
    "- Ajouter et changer le learning rate et l'ajouter à l'optimizer\n",
    "#### Essayer notre modèle sur une photo à part entière\n",
    "Pour le déploiement du modèle et pour se faire on peut créer un nouveau fichier qui fera juste cela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction : 0\n"
     ]
    }
   ],
   "source": [
    "def predict_with_model(model, img_path) : \n",
    "    image = tf.io.read_file(img_path) # On lit l'image\n",
    "    image = tf.image.decode_png(image, channels=3) # On la décode\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32) # Convertir les entiers en float, cela permet de rescaler nos images : A VOIR !\n",
    "    image = tf.image.resize(image , [60,60]) # resize les images et on a alors de la forme (60,60,3)\n",
    "    image = tf.expand_dims(image, axis=0) # pour avoir au final (1,60,60,3) pour l'adapter à notre modèle (voir summary du modèle il attend un tel format)\n",
    "\n",
    "    prediction = model.predict(image) # peut etre une décision ou un ensemble de probabilités (pour chaque classe)\n",
    "    prediction = np.argmax(prediction) # Pour avoir l'indexe de la meilleure probabilité et par conséquent le label\n",
    "    return prediction\n",
    "\n",
    "if __name__=='__main__' :\n",
    "\n",
    "    img_path = \"./Test/0/00579.png\"\n",
    "    model = tf.keras.models.load_model('./Models')\n",
    "    prediction = predict_with_model(model, img_path)\n",
    "    print(f'prediction : {prediction}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "374be8c39ae01ff66328729506a9b9a7ba9eb3f2df141c8f3098ad96d8cc6bdd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
